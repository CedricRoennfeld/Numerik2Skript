% !TeX root = ../script.tex

\section{Krylov-Raum-Methoden für EW-Probleme}
Wir verfolgen die gleiche Idee, wie auch schon bei linearen Gleichungssystemen, d.h. die ursprünglich hochdimensionalen
Probleme, werden durch geeignete Unterräume (Krylov-Räume) in kleinere Probleme umgewandelt. \\
Wir erhalten dabei ein iteratives Vorgehen, zu betrachtende Beispiele sind die Arnoldi-Methode und die 
Lanczos-Methode.\\ \\
Wir betrachten also die Eigenwertgleichung $Az=\lambda z$ mit $A\in\mathbb{C}^{n\times n}$, wobei $A$ eine sehr große
Matrix, typischerweise $n\geq 10^4$, ist.
\subsection{Galerkin-Approximation}
Eigenwertprobleme können äquivalent in Variationsform (schwache Formulierung) geschrieben werden.\\
Diese besagt: $z\in\mathbb{C}^n$ ist genau dann ein Eigenvektor von $A$ zum Eigenwert $\lambda\in\mathbb{C}$, wenn
\[\langle Az,y\rangle_2 = \lambda\langle z,y\rangle_2 \quad\forall\,y\in\mathbb{C}^n\tag{1*}\]
Diese Äquivalenz gilt, da aus $\langle r, y\rangle_2 = 0$ für alle $y\in\mathbb{C}^{n}$ folgt, dass $r=0$ sein muss, 
in unserem Fall ist $r=Az-\lambda z$ das Residuum des Eigenwertproblems.\\ \\
Sei $K_m=\Span{q^1,\dotsc,q^m}$ ein geeignet gewählter Unterrraum von $\mathbb{C}^n$ kleiner Dimension, d.h. 
$\dim K_m=m\ll n$, dann wird das $n$-dimensionale Eigenwertproblem (1*) mit dem $m$-dimensionale Eigenwertproblem
approximiert: 
\[z\in K_m, \ \lambda\in\mathbb{C}: \quad \langle Az,y\rangle_2=\lambda\langle y,z\rangle_2 \quad\forall\,y\in K_m\]
Statt alle $y\in K_m$ zu betrachten, reicht es auch, wenn wir nur die erzeugenden $q^i$ benutzen. 
Wir entwickeln die Eigenvektoren $z\in K_m$ bzgl. der gegebenen Basis:
\[z = \sum_{j=1}^{m} \alpha_j q^j\]
und erhalten somit das Galverkin-System
\[\sum_{j=1}^{k} \alpha_j \langle Aq^j,q^i\rangle_2 = \lambda\cdot\sum_{j=1}^{k} \alpha_j \langle q^j,q^i\rangle_2
\qquad\forall\,i=1,\dotsc,m\]
Wir schreiben dieses System typischerweise in kompakter Form als Eigenwertproblem 
$\mathcal{A}\alpha = \lambda\mathcal{M}\alpha$ mit Vektoren $\alpha=(\alpha_1,\dotsc,\alpha_m)$ und Matrix 
$\mathcal{A}=(\langle Aq^j, q^i\rangle_2)_{i,j=1}^m$, 
$\mathcal{M} = (\langle q^j, q^i\rangle_2)_{i,j=1}^m$. \\ \\
Im folgenden betrachten wir immer die \textit{kartesischer Representation} der Basisvektoren $q^{i}=(q_j^i)_{j=1}^n$ 
und somit schreibt man das Galerkin-EW-Problem in der Form\footnote{
    Als Erinnerung: Im Komplexen ist das Standardskalarprodukt definiert durch 
    $\langle x,y\rangle_2 = \sum_i x_i\cdot\overline{y}_i$
}
\[\sum_{j=1}^{m} \alpha_j\cdot \sum_{k,l=1}^{n} a_{k,l}\cdot q_k^{j}\cdot\overline{q^i}_l = 
\lambda\cdot \sum_{j=1}^{m} \alpha_j \cdot\sum_{k,l=1}^{n} a_{k,l}\cdot q_k^{j}\cdot\overline{q^i}_l
\quad\forall\, i=1,\dotsc,m\]
Mit $\mathcal{Q}^{(m)}=(q^,\dotsc,q^m)\in\mathbb{C}^{n\times m}$  und dem Vektor $\alpha=(\alpha_j)_{j=1}^m\in\mathbb{C}^m$ 
kann dies in der kompakten Form 
\[(\mathcal{Q}^{(m)})^*A\mathcal{Q}^{(m)}\alpha = \lambda (\mathcal{Q}^{(m)})^*\mathcal{Q}^{(m)}\alpha\]
formuliert werden. \\ \\
Wenn $\{q^1,\dotsc,q^m\}$ eine ONB von $K_m$ ist, reduziert sich dies zum normalen EW-Problem:
\[\underbrace{(\mathcal{Q}^{(m)})^*A\mathcal{Q}^{(m)}}_{=: H^{(m)}\in\mathbb{C}^{m\times m}}\alpha = 
\lambda \alpha \tag{2*}\]
Fall $H^{(m)}$ eine spezielle Struktur hat (z.\,B. Hessenberg-Matrix oder symmetrische Tridiagonalgestalt), dann kann 
das EW-Problem mit niedriger Dimension (2*) mit QR-Methode gelöst weden. \\ \\
Seine Eigenwerte können als Approximationen der dominanten Eigenwerte der ursprünglichen Matrix $A$ betrachtet werden
und werden \textit{Ritz-Eigenwerte} genannt. \\ \\
\begin{sumbox}[Krylov-Methode] \ 
    \begin{enumerate}
        \item[1.] Wähle geeignete Unterräume $K_m\in\mathbb{C}^{m\times m}$, $m\ll n$ Krylov-Raum durch Verwendung der 
        Matrix $A$ und deren Potenz.
        \item[2.] Konstruiere eine ONB $\{q^1,\dotsc, q^m\}$ von $K_m$ mit der stabilisierten Version des 
        Gram-Schmidt-Algorithmus und setze $\mathcal{Q}^{(m)}:=[q^1,\dotsc,q^m]$.
        \item[3.] Berechne die Matrix $H^{(m)}:=(\mathcal{Q}^{(m)})^*A\mathcal{Q}^{(m)}$, welche 
        konstruktionsbedingt eine Hessenberg-Matrix oder im hermitischen Fall hermitische Tridiagonalmatrix ist. 
        \item[4.] Löse das Eigenwertproblem der reduzierten Matrix $H^{(m)}\in\mathbb{C}^{m\times m}$ durch die 
        QR-Methode.
        \item[5.] Nehme die Eigenwerte von $H^{(m)}$ als die Näherung der dominanten (betragsgrößten) Eigenwerte 
        von $A$. Im Falle des kleinstgrößten Eigenwert, muss die Matrix $A^{-1}$ betrachtet werden (Konstruktion 
        der Unterräume $K_m$ kann sehr aufwendig sein).
    \end{enumerate}
\end{sumbox}
\subsection{Arnoldi-Methode}
Die Potenzmethode verwendet nur die aktuelle Iterierte $A^mq$ mit $m\ll n$ für den normierten Startvektor 
$q\in\mathbb{C}^n$ mit $\|q\|_2=1$, ignoriert aber die bereits berechneten Iterierten $\{q,Aq,A^2q,\dotsc,A^{m-1}q\}$. \\ \\
\textbf{Idee:} Verwendung dieser Informationen und Erstellen einer sogenannten \textit{Krylov-Matrix}
\[K_m = [q,Aq,A^2q,\dotsc,A^{m-1}q]\quad\text{mit }1\leq m\leq n\]
Die Spalten dieser Matrix sind nicht orthogonal zueinander. $A^tq$ konvergiert gegen den Eigenvektor zum betragsgrößten
Eigenwert, d.h. $K_m$ ist schlecht konditionert \textcolor{red}{(Was heißt schlecht konditionert bei nicht quadratisch?)} (da sich die letzten Spalten kaum ändern). \\
Die Konstruktion in eine orthogonale Basis mit dem Gram-Schmidt-Algorithmus ist instabil. \\
Wir wählen als Alternative in der Arnoldi-Methode die Verwendung einer stabilisierten Variante des 
Gram-Schmidt-Verfahrens um eine Folge orthonormaler Vektoren $\{q^1,q^2,\dotsc\}$ (bezeichnet als 
Arnoldi-Vektoren) zu erzeugen, sodass für jedes $m$ die Vektoren $\{q^1,\dotsc,q^m\}$ den Krylov-Unterraum $K_m$ 
aufspannen. 
\newpage
\begin{defbox}
Wir definieren für das Folgende orthogonalen Projektionsoperator:
\[\text{proj}_u(v) := \dfrac{\langle v,u\rangle_2}{\|u\|_2^2}\cdot u\]
Dieser projeziert den Vektor $v$ auf $\Span{u}$.
\end{defbox}
Damit ergibt sich das klassische \textit{Gram-Schmidt-Orthogonalisierungs-Verfahren} als 
\[q^{1} = \dfrac{q}{\|q\|_2},\quad\text{und}\quad\tilde{q}^{t} = A^{t-1}q - \sum_{j=1}^{t-1} \text{proj}_{q^j}(A^{t-1}q), 
\qquad q^t = \dfrac{\tilde{q}^t}{\|\tilde{q}^t\|_2}\quad\text{für } t=2,\dotsc,m\]
Der $t$-te Schritt projeziert die Komponente von $A^{t-1}q$ in Richtung der bereits bestimmten Orthogonal-Vektoren 
$\{q^1,\dotsc,q^{t-1}\}$. \\
Dies ist numerisch instabil durch Summieren der Rundungsfehler. \\ \\
Wir betrachten daher \textit{das modifizierte Gram-Schmidt-Verfahren}, wobei der $t$-te Schritt projeziert die Komponenten
von $Aq^t$ in Richtung $\{q^1,\dotsc,q^{t-1}\}$:
\[q^{1} = \dfrac{q}{\|q\|_2},\quad\text{und}\quad\tilde{q}^{t} = Aq^{t-1} - \sum_{j=1}^{t-1} \text{proj}_{q^j}(Aq^{t-1}), 
\qquad q^t = \dfrac{\tilde{q}^t}{\|\tilde{q}^t\|_2}\quad\text{für } t=2,\dotsc,m \tag{1}\]
Da $q^t$, $\tilde{q}^t$ in die gleiche Richtung zeigen und $\tilde{q}^t\perp K_t$ erhält man 
\[\langle q^t, \tilde{q}^t\rangle_2 = \|\tilde{q}^t\|_2 = 
\left\langle q^t,Aq^{t-1} - \sum_{j=1}^{t-1} \text{proj}_{q^j}(Aq^{t-1})\right\rangle_2 = \langle q^t, Aq^{t-1}\rangle_2\]
Mit $h_{i,t-1} := \langle Aq^{t-1},q^i \rangle_2$ ergibt sich mit dem modifizierte Gram-Schmidt-Algorithmus
\[Aq^{t-1}=\sum_{i=1}^{t} h_{i,t-1 }q^i, \qquad t=2,\dotsc,m+1\]
In der Praxis wird der modifizierte Gram-Schmidt-Alg. in der folgenden iterierten Form implementiert:
\begin{align*}
    q^1 &= \|q\|_2^{-1}q, \\
    q^{t,1}&=Aq^{t-1}, \\
    q^{t,j+1}&=q^{t,j}-\text{proj}_{q^j}(q^{t,j}), \tag{2}\\
    q^t&=\|q^{t,t}\|_2^{-1}q^{t,t}
\end{align*}
Man erhält das gleiche Resultat, wie beim klassischen Gram-Schmidt-Verfahren, aber mit kleinerem numerischen Fehler. \\ \\
\begin{defbox}[Arnoldi-Algorithmus] \ \\
    Für eine beliebige Matrix $A\in\mathbb{C}^{n\times n}$ bestimmt die Arnoldi-Methode eine Folge orthonormaler 
    Vektoren $q^t\in\mathbb{C}$ für $1\leq t \leq m \ll n$ (Arnoldi-Basis), durch Anwendung der modifizierten 
    Gram-Schmidt-Methode (2) auf die Basis $\{q,Aq,A^{m-1}q\}$ des Krylov-Unterraums $K_m$.
\end{defbox}
\textcolor{red}{(Algobox)} \\
Startvektor: $q^1=\|q\|^{-1}_2 q$ \\
Iteriere für $2\leq t\leq m: q^{t,1}=Aq^{t-1}$ \\
und für $j=1,\dotsc,t-1: h_{j,t} = \langle q^{t,j},q^j\rangle_2$ und $q^{t,j+1=q^{t,j}-h_{j,t}q^j}$ und $h_{t,t}=\|q^{t,t}\|_2$ 
und $q^t = h_{t,t}^{-1}\cdot q^{t,t}$ \\ \\
Bezeichne die $n\times m$-Matrix aus den ersten Arnoldi-Vektoren $\{q^1,q^2,\dotsc,q^m\}$ mit 
\[\mathcal{Q}^{(m)}:=[q^1,q^2,\dotsc,q^m]\] und sei $H^{(m)}$ die obere Hessenberg Matrix ($m\times m)$ aus $h_{j,k}$:
\[H^{(m)} = \begin{pmatrix}
    h_{11} & h_{12} & h_{13} & \dotsc & h_{1m} \\
    h_{21} & h_{22} & h_{23} & \dotsc & \vdots \\
    0 & h_{32} & h_{33} & \dotsc & \vdots \\
    \vdots & \ddots &  \ddots &   \ddots &  h_{m-1,m} \\
    0 &\dotsc & 0 & h_{m,m-1} & h_{m,m}
\end{pmatrix}\]
Die Matrizen $\mathcal{Q}^{(m)}$ sind orthonormal und mit (1) ergibt sich die Arnoldi-Beziehung 
\[A\mathcal{Q}^{(m)} = \mathcal{Q}^{(m)} H^{(m)} + h_{m,m+1}[0,\dotsc,0,q^{m+1}]\tag{3}\]
Multiplikation mit ${\mathcal{Q}^{(m)}}^*$  und Verwendung von 
\[{\mathcal{Q}^{(m)}}^* \mathcal{Q}^{(m)} = I \quad \text{und}\quad {\mathcal{Q}^{(m)}}^* q^{m+1}=0\]
ergibt 
\[H^{(m)} = {\mathcal{Q}^{(m)}}^* A \mathcal{Q}^{(m)}\]
Im Grenzfall $m=n$ ist die Matrix $H^{(n)}$ ähnlich zu $A$ und hat die gleichen Eigenwerte. \\
Dies legt nahe, dass auch für $m\ll n$ die Eigenwerte der reduzierten Matrix $H^{(m)}$ eine gute Approximation 
einiger Eigenwerte von $A$ sind. Wenn der Algorithmus endet (in exakter Arithmetik) für $m<n$ mit $h_{m,m+1}$ dann
ist der Krylov-Raum $K_m$ ein invarianter Unterraum der Matrix $A$ und die reduzierte Matrix $H^{(m)} = 
{\mathcal{Q}^{(m)}}^* A \mathcal{Q}^{(m)}$ hat $m$ Eigenwerte gemeinsam mit $A$, d.h. $\sigma(H^{(m)})\subset \sigma(A)$\footnote{Beweis: Übungsblatt}
Das folgende Lemma liefert a posteriori Abschätzungen der Genauigkeit für die Approximation der Eigenwerte von $A$ durch 
$H^{(m)}$.
\begin{thmbox}{Lemma}
    Sei $\{\mu,w\}$ ein Eigenpaar der Hessenberg-Matrix $H^{(m)}$ und sei $v=\mathcal{Q}^{(m)}w$ sodass $\{\mu,v\}$ ein 
    approximiertes Eigenpaar von $A$ ist. Dann gilt
    \[\|Av-\mu w\|_2 = |h_{m+1,m}|\cdot |w_m|,\] 
    wobei $w_m$ die letzte Komponente des Eigenvektors $w$ ist.
\end{thmbox}
\textit{Beweis.} Multiplikation von (3) mit $w$ ergibt 
\begin{align*}
Av &= A\mathcal{Q}^{(m)}w\\ 
&= \mathcal{Q}^{(m)}H^{(m)}w + h_{m+1,m}\cdot[0,\dotsc,0,q^{m+1}]w \\
&= \mu \mathcal{Q}^{(m)}w + h_{m+1,m}\cdot[0,\dotsc,0,q^{m+1}]w \\
&= \mu v + h_{m+1,m}\cdot[0,\dotsc,0,q^{m+1}]w
\end{align*}
Daraus folgt mit $\|q_{m+1}\|_2 = 1$, dass
\[\|Av-\mu v\|_2 = |h_{m+1,m}|\cdot |w_m|\]
\qed \\ \\
Dies liefert keine a priori-Information der Konvergenz der Eigenwerte von $H^{(m)}$ gegen die von $A$ für $m\to n$, aber
liefert a posteriori-Prüfung basierend auf den berechneten Größen $h_{m+q,m}$ und $w_m$, ob das erhaltene Paar 
$\{\mu,w\}$ eine gute Approximation ist.
\begin{rembox}
    Die Ritz-Eigenwerte konvergieren zu den betragsgrößten Eigenwerten von $A$. Falls die betragskleinsten Eigenwerte 
    bestimmt werden sollen, muss das diskutierte Verfahren auf die inverse Matrix angewendet werden (Vgl. Inverse 
    Iteration nach Wielandt). In diesem Fall hat man einen großen Aufwand die Krylov-Räume 
    $K_m = \Span{q,A^{-1}q,\dotsc,A^{-m+1}q}$ zu bestimmen, da hierfür die linearen Systeme $v^0:=q, Av^1=v^0, 
    \dotsc, Av^m=v^{m-1}$ gelöst werden müssen.
\end{rembox}