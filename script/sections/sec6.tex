% !TeX root = ../script.tex

\section{Minimierung von Funktionen}
Wir erinnern uns an das Gradientenabstiegsverfahren, in welchem wir die Minimierung des Funktionals 
%
\begin{align*}
  \phi(x) = \dfrac{1}{2}x^HAx - x^Hb 
\end{align*}
%
verwendet haben um die Lösung des linearen Gleichungssystems $Ax=b$ zu finden. 

Analog dazu können wir diesen Ansatz auch für beliebige Nullstellenprobleme verwenden. 

Für eine hinreichend glatte Funktion $f:D(f)\subset \R^n\to\R^m$ mit $m\geq n$, 
wandeln wir unser Nullstellenproblem $f(x)=0$ in ein nichtlineares Ausgleichsproblem um und minimieren 
%
\begin{align*}
  \phi(x) = \dfrac{1}{2}\vertn{2}{f(x)}_2^2
\end{align*}

Für eine Nullstelle $\hat{x}$ von $f$ gelten die notwendigen und hinreichenden Bedingungen eines lokalen Minimums:
%
\begin{align*}
  \nabla \phi(\hat{x}) 
  &= J_f(\hat{x})^T \cdot f(\hat{x}) = 0\\
  \nabla^2 \phi(\hat{x}) 
  &= J_f(\hat{x})^TJ_f(\hat{x}) + \sum_{i=1}^{m} f_i(\hat{x})\cdot \nabla^2 f_i(\hat{x}) 
  = J_f(\hat{x})^TJ_f(\hat{x}) \succeq 0
\end{align*}
%

Um eine potentielle Nullstelle von $f$ zu finden können wir also auch ein Minimum von $\phi$ bestimmen. Das 
bekannte Gradientenabstiegsverfahren wäre eine Möglichkeit hierfür, konvergiert jedoch nicht besonders schnell.

Eine Alternative hierzu ist die Newton-Methode oder Newton-artige Verfahren, bei welchen wir eine in jedem 
Iterationsschritt Linearisierung durchführen.

Als kurze Wiederholung betrachten wir hierfür noch einmal die Idee hinter der Newton Methode. Gegeben sei erneut 
eine hinreichend glatte Funktion $f:\R^n\to\R^n$. Für das Ziel eine Nullstelle von $f$ zu finden, betrachten wir 
die Linearisierung (Taylor-Formel 1. Ordnung)
%
\begin{align*}
  f_L(x) = f(x_0) + \dfrac{\partial }{\partial x} f(x) \Big|_{x=x_0}\cdot (x-x_0)
\end{align*}
%
Da $f(x) \approx f_L(x)$ bestimmen wir nun die Nullstelle von $f_L(x)$. Als lineares Gleichungssystem ist dies leicht 
zu lösen uns es ergibt sich 
%
\begin{align*}
  x = x_0 - \left(\dfrac{\partial }{\partial x} f(x) \Big|_{x=x_0}\right)^{-1}\cdot F(x_0)
\end{align*}
%
Aus dieser Nullstelle bilden wir eine rekursive Vorschrift und erhalten die mehrdimensionale Newton-Methode:
%
\begin{align*}
  x_{k+1} = x_k - \left(\dfrac{\partial }{\partial x} f(x) \Big|_{x=x_k}\right)^{-1}\cdot F(x_k),
  \qquad x_0\in\R^n
\end{align*}
%

\subsection{Gauß-Newton-Verfahren}
Unter Verwendung der gleichen Idee betrachten wir nun wieder unser initiales Ausgleichsproblem, 
die Minimierung von $\phi(x) = \frac{1}{2}\|f(x)\|_2^2$, und approximieren $f$ durch die Linearisierung 
$f_L$ an $x^{(k)}$:
%
\begin{align*}
  \phi(x) \approx \dfrac{1}{2} \left\| f(x^{(k)}) + f'(x^{(k)})(x - x^{(k)}) \right\|_2^2
\end{align*}
%
Wir haben daher nun ein lineares Ausgleichsproblem, welches leichter zu lösen ist. Das Minimum ist
gegeben durch
%
\begin{align*}
  x^{(k+1)} = x^{(k)} - f'(x(k))^\dagger f(x^{(k)})
\end{align*}
%
dabei beschreibt $A^\dagger$ das Pseudoinverse einer Matrix $A$.

Das resultierende Itertionsverfahren heißt Gauß-Newton-Verfahren.

\begin{colbox}{Bemerkung}
  Wenn keinen vollen Spaltenrang hat, so muss das lineare Ausgleichsproblem nicht eindeutig lösbar sein.
  $x^{(k+1)}$ gibt dann die Lösung an, welche zu $x^{(k)}$ den kleinsten euklidischen Abstand hat.
\end{colbox}

Bei der von uns gewählten Iterationsvorschrift kann es vorkommen, dass unsere lineare Annäherung
nicht genau genug ist und wir so nicht gegen eine Lösung des ursprünglich nicht-linearen Ausgleichsproblem konvergieren. 
Wir beheben dieses Problem, indem wir nicht alle Werte für $x^{(k+1)}$ erlauben,
sondern nur diese welcher in einem bestimmten Radius um $x^{(k)}$ liegen und erhalten so ein neues
Verfahren:

\subsection{Levenberg-Marquardt-Verfahren}
Da die Linearisierung $f_L$ nicht global geeignet ist sondern nur um $x^{(k)}$ herum, verschärfen wir unser 
Ausgleichsproblem durch eine Nebenbedingung und erhalten:
\begin{align*}
  \text{Minimiere} \quad \frac{1}{2} \left\| f(x^{(k)}) + f'(x^{(k)})(x - x^{(k)}) \right\|_2^2 \quad \text{unter} \quad \left\| x - x^{(k)} \right\|_2 \leq \delta_k
\end{align*}
Damit liegt unsere nächste Iterierte $x^{(k+1)}$ innerhalb einer kompakten Trust-Region
\begin{align*}
  R_k = \left\{ x \in \mathbb{R}^n \mid \|x - x^{(k)}\|_2 \leq \delta_k \right\}
\end{align*}
Durch die Schreibweise
\begin{align*}
  A_k = f'(x^{(k)}), \quad h = x - x^{(k)}, \quad b_k = -f(x^{(k)})
\end{align*}
erhalten wir eine uns vertrautere Notation:
\begin{align*}
  \text{Minimiere} \ \Psi(x) = \frac{1}{2} \|A_k h - b_k\|_2^2 \quad \text{unter} \quad \|h\|_2 \leq \delta_k
\end{align*}
eine Lösung $h^{(k)}$ liefert dann die nächste Iterierte durch $x^{(k+1)} = x^{(k)} + h^{(k)}$.

Ohne Beweis genügt eine solche Lösung $h^{(k)}$ der Gleichung
\begin{align*}
  (A_k^H A_k + \lambda_k I) h = A_k^H b_k
\end{align*}
für ein $\lambda_k \geq 0$. Weiter gilt, dass $\lambda_k$ genau dann positiv ist, wenn $\|h^{(k)}\|_2^2 = \delta_k$.