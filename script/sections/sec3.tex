% !TeX root = ../script.tex
\section{Eigenwertprobleme}
\subsection{Einleitung}
Aus der linearen Algebra ist das klassische Eigenwertproblem bekannt. Gegeben sei eine Matrix 
$A\in\mathbb{K}^{n\times n}$ und gesucht sind $\lambda\in\mathbb{K}$ und $v\in\mathbb{K}^n$, $v\neq 0$ 
sodass $Av=\lambda v$. Das Umstellen des Eigenwertproblems ergibt das System $(A-\lambda I)v=0$ (*). Hierbei muss 
$A-\lambda I$ singulär sein, sonst ist die einedeuige Lösung des Systems gegeben durch $v=0$.\\
Per Hand würden wir hier nun das charakteristische Poylnom $\chi_A(\lambda)=\det(A-\lambda I)$ aufstellen und 
dessen Nullstellen bestimmen, da dies genau die Werte für $\lambda$ sind, für welche das obige System nicht-triviale
Lösungen hat. \\
Für die nuerische Berechung der Eigenwerte ist dies nicht ratsam, da Nullstellenbestimmung bei Polynomen 
hochgradig schlecht konditioniert ist. \\
Wir stellen folgende Zusammenhänge der Berechnung von Eigenwerten und Eigenvektoren fest:
\begin{enumerate}
    \item[a)] Eigenwert-Bestimmung: Eigenvektor über LGS (*).
    \item[b)] Eigenvektor-Bestimmung: 
        Eigenwert über Rayleigh-Quotient $\lambda=\dfrac{\langle Av,v\rangle}{\|v\|_2^2}$ 
\end{enumerate}
\subsection{Einschließungssätze und Stabilität}
\begin{thmbox}{Hilfssatz}
    Seien $A,B\in\mathbb{K}^{n\times n}$ beliebige Matrizen und $\|\cdot\|$ eine natürliche Matrixnorm. Dann gilt 
    für jeden Eigenwert $\lambda$ von $A$, welcher nicht zugleich auch Eigenwert von $B$ ist, die Beziehung
    \[\|(\lambda I-B)^{-1}(A-B)\|\geq 1\]
\end{thmbox}
\textit{Beweis.} Ist $w$ ein Eigenwektor vom Eigenwert $\lambda$ von $A$, so folgt aus der Identität 
$(A-B)w = (I\lambda - B)w$, dass wenn $\lambda$ kein Eigenwert von $B$ ist, d.h. $\lambda I-B$ invertierbar: 
\[(\lambda I-B)^{-1}(A-B)w=w\]
Demnach ist also 
\[1\leq \sup_{x\in\mathbb{K}^n\backslash\{0\}} \dfrac{\|(\lambda I-B)^{-1}(A-B)x\|}{\|x\|}
=\|(\lambda I-B)^{-1}(A-B)\|\]
\subsubsection{Gerschgorin-Kreise}
\begin{thmbox}{Satz}[Satz von Gerschgorin]
    Alle Eigenwerte eine Matrix $A\in\mathbb{K}^{n\times n}$ liegen in der Vereinigung der sogenannten 
    Gerschgorin-Kreise
    \[K_j := \left\{z\in\mathbb{C} : |z-a_{jj}|\leq \sum_{k\neq j} |a_{jk}|\right\},
    \qquad \text{für } j=1,\dotsc,n.\]
    Für eine Teilmenge $I\subset\{1,\dotsc,n\}$ gilt, sind die Mengen $U=\displaystyle \bigcup_{j\in I} K_j$ 
    und $V=\displaystyle \bigcup_{j\notin I} K_j$ disjunkt, so liegen in $U$ genau $m:=|I|$ und in $V$ genau $n-m$ 
    Eigenwerte von $A$ (mehrfache Eigenwerte werden entsprechend ihrer algebraischen Vielfachheit gezählt).
\end{thmbox}
\textit{Beweis.} Zur ersten Behauptung: Wir setzen $B=diag(a_{jj})$ in dem Hilfssatz 3.1 und nehmen 
$\|\cdot\|_\infty$ als natürliche Matrixnorm. Für $\lambda\neq a_{jj}$ folgt dann 
\[\|(\lambda I -D)^{-1}(A-D)\|_\infty = \max_{j=1,\dotsc,n}\dfrac{1}{\lambda-a_{jj}}\sum_{k\neq j} |a_{jk}|\geq 1,\]
d.h. $\lambda$ liegt in einem der Gerschgorin-Kreise. \\
Für den zweiten Teil sei o.B.d.A. $I=\{1,\dotsc,m\}$. \\
Setzen wir $A_t = D + t(A-D)$, dann liegen genau $m$ Eigenwerte von $A_0=D$ in $U$ und $n-m$ 
Eigenwerte in $V$. Das selbe folgt auch für $A_1=A$, da die Eigenwerte von $A_t$ stetige Funktionen in $t$ sind. 
\qed \\ \\
Ein Alternativer Beweis zur ersten Behauptung liefert eine Betrachtung des 
Eigenwertproblems $Ax=\lambda x$ mit $x\neq 0$. Offensichtlich existiert ein $x_i$ mit $|x_j|\leq |x_i|$ für alle 
$j\neq i$. Die $i$-te Komponente von $Ax$ ist gegeben durch 
\[\lambda x_i = (Ax)_i = \sum_{j=1}^{m} a_{jj}x_j \]
Somit folgt 
\[|\lambda-a_{ii}| = \left|\sum_{j\neq i} a_{ij} \dfrac{x_j}{x_i}\right|\leq \sum_{j\neq i}|a_{ij}|\]
Demnach liegt $\lambda\in K_i$.\qed
\begin{egbox}
    Gegeben sei die Matrix 
    \[A = \begin{pmatrix}
        1 & 0.1 & -0.2 \\ 0 & 2 & 0.4 \\ -0.2 & 0 & 3
    \end{pmatrix}\]
    Es ergeben sich die folgenden Gerschgori-Kreise:
    \begin{align*}
        K_1 = \{z\in\mathbb{C} : |z-1|\leq 0.3\} \\
        K_2 = \{z\in\mathbb{C} : |z-2|\leq 0.4\} \\
        K_3 = \{z\in\mathbb{C} : |z-3|\leq 0.2\} \\
    \end{align*}
    \begin{center}
        \begin{tikzpicture}[scale=3]
        % Axes
        \draw[->] (-0.2,0) -- (4,0) node[right] {Re\((z)\)};
        \draw[->] (0,-1) -- (0,1) node[above] {Im\((z)\)};
        
        % Circle 1
        \draw[thick] (1,0) circle (0.3);
        \filldraw (1,0) circle (0.02) node[below] {\(1\)};
        \draw[thick] (1,0) -- ++(70:0.3);
        \node at ($(1,0)+(70:0.4)$) {\(r = 0.3\)};
        
        % Circle 2
        \draw[thick] (2,0) circle (0.4);
        \filldraw (2,0) circle (0.02) node[below] {\(2\)};
        \draw[thick] (2,0) -- ++(70:0.4);
        \node at ($(2,0)+(70:0.5)$) {\(r = 0.4\)};
        
        % Circle 3
        \draw[thick] (3,0) circle (0.2);
        \filldraw (3,0) circle (0.02) node[below] {\(3\)};
        \draw[thick] (3,0) -- ++(70:0.2);
        \node at ($(3,0)+(70:0.3)$) {\(r = 0.2\)};

        \end{tikzpicture}
    \end{center}
\end{egbox}
\newpage
\subsubsection{Stabilität von Eigenwerten}
\begin{thmbox}{Satz}[Stabilitätssatz]
    Sei $A\in\mathbb{K}^{n\times n}$ eine Matrix, zu der es $n$ linear unabhängige Eigenvektoren gibt 
    $\{w^{(1)},\dotsc,w^{(n)}\}$ und sei $B\in\mathbb{K}^{n\times n}$ eine zweite Matrix. Dann gibt es zu 
    jedem Eigenwert$\lambda(B)$ von $B$ einen Eigenwert $\lambda(A)$ von $A$, sodass mit der Matrix 
    $W=(w^{(1)}|\dotsc|w^{(n)})$ gilt
    \[|\lambda(A)-\lambda(B)|\leq\Cond_2(W)\cdot\|A-B\|_2\]
\end{thmbox}
\textit{Bewies.} Die Eigenwertgleichungen $Aw^{(i)}=\lambda_i(A)w^{(i)}$ lassen sich in der Form 
$AW=W\cdot\diag(\lambda_i(A))$ schreiben, d.h. $A=W\cdot\diag(\lambda_i(A))\cdot W^{-1}$ ist ähnlich zu der 
Diagonalmatrix $\Lambda = \diag(\lambda_i(A))$. Wenn nun $\lambda=\lambda(B)$ kein Eigenwert von $A$ ist, so gilt 
\[\|(\lambda I - A)^{-1}\|_2 = \|W(\lambda I - \Lambda)^{-1}W^{-1}\|_2 
\leq \|W\|_2\cdot\|W^{-1}\|_2\cdot\|(\lambda I-\Lambda)^{-1}\| 
= \Cond_2(W)\cdot \max_{i=1,\dotsc,n} |\lambda-\lambda_i(A)|^{-1}\]
Mit dem Hilfssatz 3.1 folgt dann die Behauptung. \qed \\ \\
Für hermitische Matrizen $A\in\mathbb{K}^{n\times n}$ existiert bekannterweise eine Orthonormalbasis des 
$\mathbb{K}^{n\times n}$ aus Eigenvektoren, sodass die Matrix $W$ als unität angenommen werden kann, 
d.h. $ww^{-*}=I$. In diesem Fall gilt $\Cond_2(W)=\|W^{-*}\|_2\cdot\|W\|_2 = 1$. \\ \\
\textbf{Regel:} Allgemein kann man sagen, dass das Eigenwertproblem für hermtische Matrizen gut konditioniert
ist, während das allgemeine Eigenwertproblem je nach Größe von $\Cond_2(W)$ beliebig schlecht konditioniert 
sein kann.
\subsection{Iterative Verfahren}
Im folgenden wollen wir ein iteratives Verfahren zu Lösung des partiellen Eigenwertproblems einer 
Matrix $A\in\mathbb{K}^{n\times n}$ betrachten.
\subsubsection{Potenz-Methode}
\begin{defbox}
    Die Potenzmethode (Von-Mises-Iteration) erzeugt ausgehend von einem Startvektor $z^{(0)}\in\mathbb{C}^n$ mit
    $\|z^0\|=1$ eine Folge von Iterationen $z^{(t)}\in\mathbb{C}^n, t=1,2,\dotsc$ durch 
    \[\tilde{z}^{(t)}=Az^{(t-1)} \quad\text{und}\quad z^{(t)} = \tfrac{\tilde{z}^{(t)}}{\|\tilde{z}^{(t)}\|}.\]
    Für einen beliebigen  Index $k\in\{1,\dotsc,n\}$, (z.B. maximale Komponente von $z^k$) wird gesetzt:
    \[\lambda^{(t)} = \dfrac{(Az^{(t)})_k}{(z^{(t)})_k}\]
    \textit{Zur Normierung wird üblicherweise $\|\cdot\|=\|\cdot\|_2$ oder $\|\cdot\|_\infty$ verwendet.} 
\end{defbox}
Zur Analyse des Verfahrens nehmen wir an, dass die Matrix $A$ diagonalisierbar ist, d.h. ähnlich zu einer 
Diagonalmatrix ist. Dies ist äquivalent zu der Tatsache, dass $A$ eine Basis von Eigenvektoren 
$\{w^{(1)},\dotsc,w^{(n)}\}$ besitzt. Weiter seien diese Eigenvektoren $w^{(i)}$ normiert. \\
Wir nehmen an, dass $z^{(0)}$ eine nicht-triviale Komponente bezüglich $w^{(n)}$ besitzt. 
(Dies ist keine wesentliche Einschränkung, da aufgrund des unvermeidbaren Rundungsfhlers dieser Fall der 
Iteration sicher einmal auftritt)
\newpage
\begin{thmbox}{Satz}[Potenz-Methode]
    Die Matrix $A$ sei diagonalisierbar und ihr betragsgrößter Eigenwert sei separiert von den anderen 
    Eigenwerten, d.h. $|\lambda_n|>|\lambda_{n-1}|\geq|\lambda_{n-2}|\geq\dotsc\geq|\lambda_1|$. \\ 
    Der Startvektor $z^{(0)}$ habe eine nicht-triviale Komponente bezüglich des zugehörigen Eigenvektors $w^{(n)}$.
    Dann gibt es Zahlen $\delta_t\in\mathbb{C}, |\delta_t|=1$, 
    sodass $\|z^{(t)}-\delta_t\cdot w^{(n)}\|\rightarrow 0$ für $t\rightarrow\infty$ und es gilt
    \[\lambda^{(t)}-\lambda_n = \mathcal{O}\left(\left|\dfrac{\lambda_{n-1}}{\lambda_n}\right|^t\right)\qquad 
    \text{ für } t\rightarrow \infty\]
\end{thmbox}
\textit{Beweis.} Sei $z^{(0)}=\sum_i \alpha_i\cdot w^{(i)}$ die Basisdarstellung des Startvektors 
(mit $\alpha_n\neq 0$). Für die Iterierten gilt:
\[z^{(t)} = \dfrac{\tilde{z}^{(t)}}{\|\tilde{z}^{(t-1)}\|} = \dfrac{Az^{(t-1)}}{\|Az^{(t-1)}\|} 
= \dotsc = \dfrac{A^tz^{(0)}}{\|A^tz^{(0)}\|}\]
Dabei gilt:
\[A^tz^{(0)} = \sum_{i=1}^{n}\alpha_i\lambda_i^tw^{(i)} = \lambda_n^t \alpha_n\cdot\left(w^{(n)} + 
\sum_{i\neq n} \dfrac{\alpha_i}{\alpha_n}\left(\dfrac{\lambda_i}{\lambda_n}\right)^t w^{(i)}\right)\]
Wegen $|\tfrac{\lambda_i}{\lambda_n}|\leq \rho:=|\tfrac{\lambda_{n-1}}{\lambda_n}|<1$ für $i=1,\dotsc,n-1$ folgt
\[A^tz^{(0)} = \lambda_n^t\alpha_n(w^{(n)}+\mathcal{O}(\rho^t))
\qquad \text{ für } t\rightarrow \infty\]
Dies ergibt:
\[z^{(t)} = \dfrac{\lambda_n^t\alpha_n(w^{(n)}+
\mathcal{O}(1))}{|\lambda_n^t\alpha_n|\cdot\|w^{(n)}+\mathcal{O}(\rho^t)\|}
= \underbrace{\dfrac{\lambda_n^t\alpha_n}{|\lambda_n^t\alpha_n|}}_{=:\delta_k}\cdot (w^{(n)}+\mathcal{O}(\rho^t))\]
Dabei ist $\delta_t\in\mathbb{C}$ und $|\delta_t|=1$, daher folgt die erste Aussage.\\
Weiter gilt 
\begin{align*}
    \lambda^{(t)} &= \dfrac{(Az^{(t)})_k}{(z^{(t)})_k} \\
    &= \dfrac{(A^{t+1}z^{(0)})_k}{\|(A^{t+1}z^{(0)})_k\|}\cdot\dfrac{{\|(A^{t+1}z^{(0)})_k\|}}{(A^tz^{(0)})_k} \\
    &= \dfrac{\lambda_n^{t+1}(\alpha_n w_{n,k}+\sum_{i\neq n} \alpha_i(\tfrac{\lambda_i}{\lambda_n})^{t+1}w_{i,k})}
    {\lambda_n^t(\alpha_n w_{n,k}+\sum_{i\neq n} \alpha_i(\tfrac{\lambda_i}{\lambda_n})^tw_{i,k})} \\
    &= \lambda_n + \mathcal{O}\left(\left|\dfrac{\lambda_{n-1}}{\lambda_n}\right|^t\right)\qquad 
    \text{ für } t\rightarrow \infty
\end{align*}
\qed \\ \\
Die Konvergenz der Potenzmethode ist umso besser, je mehr der betragsgrößte Eigenwert $\lambda_n$ von den übrigen 
betragsmäßig separiert ist. Der Beweis ist verallgemeinerbar für betragsgrößte Eigenwerte, welche merhfach 
auftreten, sofern die Matrix diagonalisierbar ist. \\ \\
\subsubsection{Inverse Iteration} 
Als nächstes wollen wir uns die \glqq{}Inverse Iteration\grqq{} nach
Wielandt anschauen. \\
Wir nehmen an, man hat bereits eine Näherung $\tilde{\lambda}$ für einen Eigenwert $\lambda_k$ der regulären Matrix 
$A$ (z.B. durch Einschließungssätze). Die Näherung sei gut in dem Sinne, 
dass $|\lambda_k-\tilde{\lambda}|\ll |\lambda_i-\tilde{\lambda}|$ für $i\neq k$.\\
Sei $\lambda$ ein Eigenwert von $A$, dann ist $\lambda^{-1}$ ein Eigenwert von $A^{-1}$. 
Wir betrachten das Eigenwertproblem, welches sich für die Matrix $A-\tilde{\lambda}I$ ergibt:
\[(A-\tilde{\lambda}I)v=\xi v \iff (A-\tilde{\lambda}I-\xi I)v = 0 \iff (A-(\tilde{\lambda}+\xi)I)v=0\]
Also ist $\xi=\lambda_k-\tilde{\lambda}$ ein Eigenwert von $A-\tilde{\lambda}I$ und folglich ist 
$\mu=\tfrac{1}{\xi}=(\lambda_k-\tilde{\lambda})^{-1}$ ein Eigenwert von $(A-\tilde{\lambda}I)^{-1}$. \\
Allgemeiner hat im Falle $\tilde{\lambda}\neq\lambda_k$ die Matrix $(A-\tilde{\lambda}I)^{-1}$ die Eigenwerte 
$\mu_i = (\lambda_i-\tilde{\lambda})^{-1}$ für $i=1,\dotsc,n$ un des gilt 
\[\left|\dfrac{1}{\lambda_k-\tilde{\lambda}}\right| \gg \left|\dfrac{1}{\lambda_i-\tilde{\lambda}}\right|\qquad 
\text{für } i\neq k\]
\begin{defbox}
    Die inverse Iteration besteht in der Anwendung der Potenzmethode auf die Matrix $(A-\tilde{\lambda}I)^{-1}$
    mit einer a priori Schätzung $\tilde{\lambda}$ zum gesuchten Eigenwert $\lambda_k$. \\ 
    Ausgehend von einem Startwert $z^{(0)}$ werden Iterierte $z^{(t)}$ bestimmt als Lsg. der Gleichungssysteme
    \[(A-\tilde{\lambda}I)z^{(t)} = \tilde{z}^{(t-1)},
    \qquad z^{(t)} = \dfrac{\tilde{z}^{(t)}}{\|\tilde{z}^{(t)}\|}\]
    Die zugehörige Eigenwertnäherung wird bestimmt durch 
    \[\mu^{(t)}=\dfrac{(z^{(t)})_k}{((A-\tilde{\lambda}I)z^{(t)})_k}\]
    mit Nenner $\neq 0$ (oder im symmetrischen Fall mit Hilfe der Rayleigh-Quotienten).
\end{defbox}
Aufgrund der Aussagen über Potenzmethoden liefert die inverse Iteration also für eine diagonalisierbare Matrix
jeden Eigenwert, zu dem bereits eien hinreichend gute Näherung bekannt ist.
\subsection{Page-Rank-Algorithmus}
Das Ziel des Page-Rank-Algorithmus ist die Bestimmung der Ausgabereihenfolge bei Suchergebnissen. Dabei berufen 
wir uns auf folgende Regeln:
\begin{enumerate}
    \item[(1)] Eine Website erhält eine umso höhere Bewertung, je mehr Links auf sie zeigen.
    \item[(2)] Links von höher bewerteten Websites soll relevanter sein, als solceh von unbedeutenden
    \item[(3)] Ein Link von einer Website, die wenig Links nach außen hat, soll höher gewichtet werden als der
        von einer Website mit vielen Links nach außen. 
\end{enumerate}
Wir beschreiben unser Model als ein Netz mit $n$ Seiten, wobei ein Index $k$ immer für eine Seite steht. \\
Gesucht ist die Bedeutung einer Seite $x_k\in\mathbb{R}$ \\
$L_k$ sei die Menge der Seiten, die auf $k$ verlinken, Links auf Seiten von sich selbst werden dabei nicht
berücksichtigt. \\
$n_k$ sei die Anzahl der Links, der Webite $k$ nach außen.\\
Wir modellieren mittels folgendem LGS
\[x_k = \sum_{j\in L_k}\tfrac{1}{n_j}\cdot x_j\]
Die Gleichung $x=Ax$ entspricht hierbei der Eigenwertgleichung für den Eigenwert $\lambda=1$. \\
Der historische Ansatz von Google ist die Potenzmethode:
\[x = \begin{pmatrix}
    x_1 \\ \vdots \\ x_n
\end{pmatrix}, \qquad A_{ij} = a_{ij} = \begin{cases}
    \tfrac{1}{n_j}, & \text{falls die Seite } j \text{ auf die Seite } i \text{ verlinkt} \\
    0, & \text{sonst}
\end{cases}\]
\newpage
\subsubsection{Stochastische Vektoren/Matrizen}
\begin{defbox}
    Ein Vektor $p\in\mathbb{R}^n$ heißt stochastischer Vektor, wenn alle Elemente $p_i$ nicht-negativ sind und die 
    Summe der Elemente des Vektors gleich 1 ist, d.h. $\sum_{i} p_i = 1$. \\
    Eine Matrix $A\in\mathbb{R}^{n\times n}$ heißt stochastische Matrix, wenn ale Spalten der Matrix stochastische 
    Vektoren sind, d.h.
    \[a_{ij}\geq 0\ \forall i,j \quad \text{und } \quad \sum_{i=1}^n a_{ij}=1\ \forall j\]
\end{defbox}
\begin{thmbox}{Lemma}
    Sei $A\in\mathbb{R}^{n\times n}$ eine stoch. Matrix und $p\in\mathbb{R}^{n}$ ein stoch. Vektor, dann ist 
    das Produkt $Ap\in\mathbb{R}^{n\times n}$ wieder ein stoch. Vektor.
\end{thmbox}
\textit{Beweis.} Es sei $a_i$ die $i$-te Spalte der Matrix $a_i$, d.h. $a_i$ ist ein stoch. Vektor
\begin{align*}
    A\cdot p = A\cdot \begin{pmatrix}
    p_1 \\ \vdots \\ p_n
\end{pmatrix} &= p_1\cdot a_1 + \dotsc + p_n\cdot a_n \\
&= \sum_{i} (p_1a_{i1} + \dotsc + p_na_{in}) \\
&= p_1\sum_{i}a_{i1} + \dotsc +  p_n\sum_{i}a_{in} \\
&= p_1\cdot 1 + \dotsc + p_n\cdot n = 1
\end{align*}
Weiter gilt offensichtlich $(Ap)_{ij}\geq 0$.\qed
\begin{thmbox}{Lemma}
    Seien $A,B\in\mathbb{R}^{n\times n}$ stoch. Matrizen, dann ist das Produkt $A\cdot B$ wieder eine stoch. Matrix.
\end{thmbox}
\textit{Beweis.} Folgt direkt aus Lemma 3.9.
\begin{thmbox}{Satz}
    Eine stochastische Matrix $A$ hat immer den Eigenwert $1$. Der Betrag aller anderen Eigenwerte ist
     kleiner oder gleich 1.
\end{thmbox}
\textit{Beweis.} Für den ersten Teil nutzen wir aus, dass $A$ und $A^T$ die gleichen Eigenwerte, da $A$ und $A^T$ die 
gleiche Determinante besitzten und damit die charakteristischen Polynome $\chi_A(\lambda) = \det(A-\lambda I) 
= \det(A^T-\lambda I) = \chi_{A^T}(\lambda)$. \\
Weiter ist die Summe der Elemente jedes Zeilenvektors von $A^T$ ist gleich 1 (da $A$ stoch.), 
somit ist $e=(1,\dotsc,1)^T$ ein Eigenvektor von $A^T$ mit Eigenwert $1$. Somit besitzt auch die Matrix $A$ den 
Eigenwert $\lambda = 1$. \\
Angenommen es existiert ein Eigenvektor $v$ zum Eigenwert $\lambda$ mit $|\lambda|>1$, dann gilt
\[A^n v = A^{n-1}(Av) = A^{n-1}\lambda v = \lambda A^{n-1}v = \dotsc = \lambda^n v\]
Für die Länge dieses Vektors gilt $\|\lambda^n v\| = |\lambda^n|\cdot \|v\|$ ein exponentieller Wachstum in $n$, da 
$|\lambda|>1$. \\
Daraus folgt, dass für große $n$ ein Element $(A^n)_{ij}$ exisitert, welches größer als $1$ ist.\\
Da nach Lemma 3.10 die Matrix $A^n$ stoch. ist bildet dies einen Widerspruch.\qed
\begin{thmbox}{Lemma}
    Die Bewertungsmatrix $A$ des Page-Rank-Algorithmus ist eine stoch. Matrix.
\end{thmbox}
\textit{Beweis.} Offensichtlich gilt $a_{ij}\geq 0$, weiter gilt
\[\sum_{i=1}^n a_{ij} = n_j\cdot \dfrac{1}{n_j} + (n-n_j)\cdot 0 = 1\]
\qed
\begin{egbox}
    Wir betrachten ein einfaches Netz mit $4$ Knoten:  \\
    \begin{center}
        \begin{tikzpicture}[>=Stealth, node distance=2.5cm, every node/.style={circle, draw, minimum size=1cm}, auto]

        % Knoten
        \node (1) at (0,2.5) {1};
        \node (2) at (0,0) {2};
        \node (3) at (2.5,2.5) {3};
        \node (4) at (2.5,0) {4};

        % Kanten
        \draw[->, bend left=15] (1) to (3);
        \draw[->, bend left=15] (3) to (1); 
        \draw[->, bend left=15] (1) to (4); 
        \draw[->, bend left=15] (4) to (1); 
        \draw[->] (1) -- (2);
        \draw[->] (2) -- (4);
        \draw[->] (4) -- (3);
        \draw[->] (2) -- (3);

        \end{tikzpicture}
    \end{center}
    Es ergibt sich folgendes Gleichungssystem:
    \[\begin{pmatrix}
        x_1\\ x_2\\ x_3\\ x_4
    \end{pmatrix} = \begin{pmatrix}
        0 & 0 & 1 & 1/2 \\ 
        1/3 & 0 & 0 & 0 \\
        1/3 & 1/2 & 0 & 1/2 \\
        1/3 & 1/2 & 0 & 0
    \end{pmatrix}\cdot \begin{pmatrix}
        x_1\\ x_2\\ x_3\\ x_4
    \end{pmatrix}\]
    Lösen dieses linearen Gleichungssystems liefert:
    \[x \in \text{span}\left\{\begin{pmatrix}
        0.72 \\ 0.24 \\ 0.54 \\ 0.36
    \end{pmatrix}\right\}\]
    Demnach hat die erste Website die höchste Bewertung. LOL
\end{egbox}