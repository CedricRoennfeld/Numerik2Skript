\documentclass{article}
\hfuzz=1000pt
\hbadness=10000
\usepackage{scriptStyle}

\begin{document}

\section{Wiederholung?}
Wir starten mit einer kurzen Wiederholung zur Fixpunktiteration zum Lösen von Gleichungen der Form $Tx=x$ durch $x_{n+1}=Tx_n$.

\begin{thmbox}{Satz}[Banach 1922]
    Sei $M$ eine abgeschlossene nichtleere Teilmenge in einem vollständig metrischem Raum $(X,d)$. Sei $T:M\rightarrow M$ eine Selbstabbildung und $k$-kontraktiv, d.h. $d(Tx,Ty)\leq k\cdot d(x,y)\ \forall x,y\in M$ mit $0\leq k < 1$. Dann folgt:
    \begin{enumerate}
        \item Existenz und Eindeutigkeit: die Gleichung $Tx=x$ hat genau eine Lösung, d.h. $T$ hat genau einen Fixpunkt in M.
        \item Konvergenz der Iteration $x_{k+1}=Tx_k$. Die Folge $(x_k)_{k\in\mathbb{N}}$ konvergiert gegen den Fixpunkt $x^*$ für einen beliebigen Startpunkt $x_0\in M$.
        \item Fehlerabschätzung: Für alle $n=0,1,\dotsc$ gilt 
        \begin{itemize}
            \item a-priori: $d(x_n,x^*)\leq k^n(1-k)^{-1}d(x_0,x_1)$
            \item a-posteriori: $d(x_{n+1},x^*)\leq k(1-k)^{-1}d(x_n,x_{n+1})$
        \end{itemize}
        \item Konvergenzrate: Für alle $n\in\mathbb{N}$ gilt $d(x_{n+1},x^*)\leq k\cdot d(x_n,x^*)$
    \end{enumerate}
\end{thmbox}
\textit{Beweis.} 
\begin{enumerate}
    \item[2.] Wir zeigen, dass $(x_n)$ eine Cauchy-Folge ist. Für den Abstand zweier benachbarter Folgeglieder $x_n$ und $x_{n+1}$ gilt
    \[d(x_n,x_{n+1})=d(Tx_{n-1},Tx_n)\leq k\cdot d(x_{n-1},x_n)\leq \dotsc \leq k^n\cdot d(x_0,x_1)\]
    Mehrfache Anwendung der Dreiecksungleichung liefert daher für $n,m\in\mathbb{N}$:
    \begin{align*}
        d(x_n,x_{n+m}) &\leq d(x_n,x_{n+1}) + d(x_{n+1},x_{n+2}) + \dotsc + d(x_{n+m-1}, x_{n+m}) \\
        & \leq (k^n + k^{n+1} + \dotsc + k^{n+m})\cdot d(x_0,x_1) \\
        & \leq k^n(1+k+k^2+\dotsc)\cdot d(x_0,x_1) \\
        & = k^n\cdot(1-k)^{-1}d(x_0,x_1)
    \end{align*}
    Demnach folgt $d(x_n,x_{n+m})\rightarrow 0$ für $n\rightarrow \infty$ und da $X$ vollständig ist konvergiert $(x_n)$ gegen ein $x^*\in X$.
    \item[1.] Da $T$ stetig ist (aufgrund $k$-Kontraktivität) folgt für die konvergente Folge $(x_n)$, dass 
    \[x^* = \lim_{n\rightarrow\infty} x_{n+1} = \lim_{n\rightarrow\infty} Tx_n = Tx^*\]
    Da $M$ abgeschlossen ist existiert also ein Fixpunkt in $M$. \\
    Dieser ist eindeutig, denn für $x,y$ mit $Tx=x$ und $Ty=y$ gilt $d(x,y)=d(Tx,Ty)\leq k d(x,y)$, also $d(x,y)=0$.
    \item[3.] Aus dem Beweis zu 2. haben wir $d(x_n,x_{n+m})\leq k^n(1-k)^{-1}d(x_0,x_1)$, wegen der Stetigkeit der Metrik folgt die a-priori-Fehlerabschätzung aus $m\rightarrow\infty$. \\
    Die a-posteriori-Fehlerabschätzung folgt analog aus dem Ansatz
    \begin{align*}
        d(x_{n+1},x_{n+1+m}) &\leq d(x_{n+1},x_{n+2}) + \dotsc + d(x_{n+m}, x_{n+1+m}) \\
        & \leq (k + \dotsc + k^{m})\cdot d(x_n,x_{n+1}) \\
        & \leq k\cdot(1-k)^{-1}d(x_n,x_{n+1})
    \end{align*}
    \item[4.] Folgt direkt durch $d(x_{n+1},x^*)=d(Tx_n,Tx^*)\leq k\cdot d(x_n,x^*)$ 
\end{enumerate}

\begin{egbox} 
    Wir betrachten das Nullstellenproblem $f:\mathbb{R}\rightarrow\mathbb{R}, x\mapsto \cos x - x = 0$. \\
    Umformung ergibt $\underbrace{\cos x}_{Tx} = x$ und somit die Fixpunktiteration $x_{k+1}=Tx_k=\cos(x_k)$ \\
    \textcolor{red}{Abb. 1.1} \\
    \underline{Prüfung der Voraussetzungen des Banach'schen FP-Satzes:} \\
    Wir wählen als Einschränkung $M=[0,1]$, dies liefert uns eine Selbstabbildung auf einer abgeschlossenen Teilmenge $M$ des vollständig metrischen Raum $\mathbb{R}$ mit der Abstandsfunktion $ d(x,y) = |x-y|$. \\
    Weiter ist die Abbildung $k$-kontraktiv:  Nach Mittelwertsatz der Differentialrechnung gilt 
    \[|\cos x - \cos y| = \underbrace{|\sin \xi|}_{\leq \sin(1)}\cdot|x-y|\leq \underbrace{0,85}_{=:k}\cdot |x-y|, \quad \text{für } \xi\in[0,1]\]
    Wir können also nach Banach die Existenz und Eindeutigkeit eines Fixpunkt $x^*$ folgern, diesen Fixpunkt finden wir durch die konvergente Folge $x_{k+1}=\cos x_k$. 
\end{egbox}
Wir betrachten im folgenden die Idee der Umwandlung eines Nullstellenproblems in Fixpunkt-Gleichung noch etwas allgemeiner. Für eine Gleichung $f(x)=0$ mit $f:\mathbb{R}\rightarrow\mathbb{R}$ haben wir verschiedene Möglichkeiten zur Umformung:\\
\begin{enumerate}
    \item[a)] Betrachte $Tx := x-f(x)$ gefolgert aus $f(x)=0\Leftrightarrow -f(x)=0 \Leftrightarrow x-f(x)=x$.
    \item[b)] Betrachte $Tx := x-\omega \cdot f(x)$ mit $\omega\neq 0$ (lineare Relaxation)
    \item[c)] Betrachte $Tx:=x-\omega \cdot g(f(x))$ mit $\omega \neq 0$ und geeigneter Funktion $g$ (nichtlineare Relaxation). Wenn $g(0)\neq 0$ dann betrachte $Tx:=x-\omega\cdot(g(f(x))+g(0))$
    \item[d)] Betrachte $Tx:=x-(f'(x))^{-1} f(x)$ (Newtonverfahren) \\
    Newton hat teils Probleme, bei falschen Startwerten: \\
    \textcolor{red}{Abb 1.2}
    \item[e)] Betrachte $Tx:=h^{-1}(f(x)-g(x))$, wobei $f(x)=h(x)+g(x)$ (Splitting-Verfahren) 
\end{enumerate}
\section{Iteratives Vorgehen zur Lösung linearer Gleichungssysteme}
\subsection{Splittingverfahren}
Gegeben sei das LGS $Ax=b$ für $A\in\mathbb{K}^{n\times n}, b\in\mathbb{K}^n, x\in\mathbb{K}^n$, wobei $\mathbb{K}\in\{\mathbb{R}, \mathbb{C}\}$. Wir wollen dieses LGS nun in ein FP-Problem umformen, sei hierfür $A$ nicht singulär (sonst nicht lösbar). \\
Wir schreiben $A=M-N$, wobei $M$ invertierbar und häufig sogar eine Diagonalmatrix ist (damit $M$ leicht zu invertieren ist). Dies liefert:
\[Ax = b \Leftrightarrow (M-N)x=b \Leftrightarrow Mx=Nx+b x=\underbrace{M^{-1}\cdot(Nx+b)}_{\Tilde{T}x}\]
$\Tilde{T}$ ist affin-linear. Wir erhalten also unser FP-Problem $x=\Tilde{T}x=Tx+c$ mit $T=M^{-1}N$ und $c=M^{-1}b$ \\ \\

\algobox{Splittingverfahren}{
\begin{algorithm}[H]
    \SetAlgoNoLine
    \InitTe{$A=M-N$ mit $N\in GL(n,\mathbb{K})$}{}
    \SetAlgoLined
    Wähle $x^{(0)}\in\mathbb{K}^n$ beliebig \\
    \ForTe{$k=1,0,\dotsc$}{
        löse $Mx^k=Nx^{k-1}+b$} 
    \SetAlgoNoLine
    \UntilTe{stop (beliebiges Stopkriterium)}{}
\end{algorithm}
}
Konvergenz dieses Algorithmus folgt aus Banachschen Fixpunktsatz.

\begin{rembox}
    Nach gleicher Überlegung lässt sich auch unser obiges Splittingverfahren für Nullstellenbestimmung herleiten:
    \[f(x)=0\Leftrightarrow h(x)+g(x):=f(x) = 0 \Leftrightarrow h(x)=f(x)-g(x) \Leftrightarrow x=h^{-1}(f(x)-g(x))\]
\end{rembox}
\textit{Wiederholung:} Eine Matrixnorm ist eine Norm auf dem Vektorraum der Matrizen, d.h. $\|\cdot\|:\mathbb{K}^{n\times n}\rightarrow \mathbb{R}$, bereits bekannte Matrixnormen sind:
\begin{itemize}
    \item Frobeniusnorm: $\|A\|_F := \left(\displaystyle \sum_{i,j}|a_{ij}|^2\right)^{1/2}$
    \item Spaltensummennorm $\|A\|_1:=\max_j \sum_i |a_{ij}|$
    \item Zeilensumennorm $\|A\|_\infty:=\max_i \sum_j |a_{ij}|$
    \item Spektralnorm $\|A\|_2:=\sqrt{\lambda_{max}(A^HA)}$, \qquad $(A^H := \overline{A}^T)$
\end{itemize}
Im allgemeinen induziert eine Vektornorm auch immer eine Matrixnorm, diese nennen wir auch Operatornorm:
\[\|A\|:=\max_{\|x\|=1}\|Ax\|\]
Die oben aufgelisteten Normen $\|\cdot\|_1,\|\cdot\|_2$ und $\|\cdot\|_\infty$ sind die Operatornormen zu der jeweiligen $p$-Normen. \\ \\
Eine Norm $\|\cdot\|$ auf $\mathbb{K}^{n\times n}$ heißt submultiplikativ, falls $\|AB\|\leq\|A\|\cdot\|B\|$ und sie heißt verträglich mit einer Vektornorm $\|\cdot\|_V$, falls $\|Ax\|_V\leq \|A\|\cdot\|x\|_V$. \\
Operatornormen sind immer submultiplikativ und verträglich zu der Vektorrnorm, aus welcher sie abgeleitet wurden.
\begin{thmbox}{Satz}
    Ist $\vertiii{\cdot}$ eine Norm auf $\mathbb{K}^{n\times n}$, die mit einer Vektornorm verträglich ist, und ist $\vertiii{M^{-1}N}<1$, dann konvergiert der Algorithmus für jedes für jedes $x^{(0)}\in\mathbb{K}^n$ gegen $A^{-1}b$, d.h. gegen die Lösung des linearen Gleichungssystems $Ax=b$.
\end{thmbox}
\textit{Beweis.} Sei $\tilde{T}(x) := Tx + c$ mit $T=M^{-1}N$ und $c=M^{-1}b$.\\
Offensichtlich gilt $\tilde{T}:\mathbb{K}^n\rightarrow\mathbb{K}^n$, sowie 
\[\|\tilde{T}(x)-\tilde{T}(y)\| = \|Tx-Ty\|\leq \|T\|\cdot\|x-y\|\]
Da $\|T\|=\|M^{-1}N\|<1$ ist $\tilde{T}$ eine $k$-kontraktive Selbstabbildung und somit konvergiert die Folge $(x^k)$ aus dem Algorithmus gegen den eindeutigen Fixpunkt $x^*$ mit $\tilde{T}(x^*)=x^*$. \\ 
Einsetzen der Definition von $\tilde{T}$ liefert:
\[x^*=Tx+c=M^{-1}(Nx+b)\Rightarrow Mx=Nx+b \Rightarrow Ax=(M-N)x=b\]
\begin{thmbox}{Korollar}
    Sei $A$ invertierbar, so konvergiert der obige Algorithmus genau dann für alle Startwerte $x^{(0)}\in\mathbb{K}^n$ gegen $x^*=A^{-1}b$, wenn für den Spektralradius $\rho(T)=\max\{|\lambda|:\lambda\in\sigma(T)\}$ die Ungleichung $\rho(T)<1$ erfüllt ist.
\end{thmbox}
\textit{Beweis.} \\
$\Leftarrow:$ Falls $\rho(T)<1$ dann existiert eine Norm $\|\cdot\|_\varepsilon$ auf $\mathbb{K}^n$ und eine dadurch induzierte Operatornorm $\vertiii{\cdot}_\varepsilon$ auf $\mathbb{K}^{n\times n}$ mit $\vertiii{T}_\varepsilon \leq \rho(T) + \varepsilon < 1$ \textcolor{red}{Warum?} \\
Satz 2.2 liefert dann die Konvergenz des Algorithmus. \\ \\
$\Rightarrow:$ Angenommen $\rho(T)\geq 1$, d.h. es existiert ein Eigenwert $\lambda$ von $T$ mit $|\lambda|\geq 1$ und zugehörigem Eigenvektor $z$. Für $x^{(0)}=x^*+z$ und festes $k$ sich der Iterationsfehler
\[x^{(k)}-x^* = Tx^{(k-1)}+c-x^* = Tx^{(k-1)}-Tx^* = T(x^{(k-1)}-x^*)\]
Induktiv folgt dann $x^{(k)}-x^* = T^k(x^{0}-x^*)=T^kz=\lambda^kz$, demnach gilt 
$\|x^{(k)}-x^*\|=|\lambda^k|\cdot\|z\|$.
Für größer werdendes $k$ kann $x^{(k)}$ also nicht gegen $x^*$ konvergieren. \\ \\
\begin{thmbox}{Satz}
    Unter gleichen Voraussetzungen des obigen Korollars gilt 
    \[\max_{x^{(0)}\in\mathbb{K}^n} \limsup_{k\rightarrow \infty} \|x^*-x^{(k)}\|^{1/k}=\rho(T)\]
\end{thmbox}
\textit{Beweis.}
Aus dem Beweis von Korollar 2.3 sehen wir
\[\max_{x^{(0)}\in\mathbb{K}^n} \limsup_{k\rightarrow \infty} \|x^*-x^{(k)}\|^{1/k}\geq\limsup_{k\rightarrow\infty} \|T^kz\|^{1/k}=\limsup_{k\rightarrow\infty}|\lambda|\cdot\|z\|^{1/k}=|\lambda|=\rho(T)\]
Für jeden Startwert $x^{(0)}\in\mathbb{K}^n$ gilt nun 
\[\|x^{(k)}-x^*\|_\varepsilon = \|T^k(x^{(0)}-x^*)\|_\varepsilon\leq \|T\|_\varepsilon^k\cdot \|x^{(0)}-x^*\|_\varepsilon\]
Da im $\mathbb{K}^n$ alle Normen äquivalent sind, also inbesondere auch $\|\cdot\|_\varepsilon$ und $\|\cdot\|$, exisitert eine Konstante $c_\varepsilon>0$, so dass
\[\|x^{(k)}-x^*\|^{1/k}\leq\left(c_\varepsilon\cdot\|x^{(k)}-x^*\|_\varepsilon\right)^{1/k}\leq \|T\|_\varepsilon\cdot\left(c_\varepsilon\cdot\|x^{(0)}-x^*\|_\varepsilon\right)^{1/k}\xrightarrow{k\rightarrow\infty} \|T\|_\varepsilon\]
Folglich ist 
\[\varrho(T) \le \max_{x^{(0)}} \limsup_{k\rightarrow \infty} \|x^{(k)}-x^*\|^{1/k}\le \vertiii{T}_\varepsilon\]
\qed
Dieser Satz ermöglicht es nun einen sinnvollen Begriff der Konvergenzrate zu definieren:
\begin{defbox} \\
    Die Zahl $\varrho(T)$ heißt (asymptotischer) Konvergenzfaktor von der Iteration $x^{(k)}=Tx^{(k-1)}+c$. \\
    Die (asymptotische) Konvergenzrate lässt sich dadurch ausdrücken mit $r=-\log_{10}\varrho(T)$
\end{defbox}
Mittels der Zerlegung $A=D+L+R$, wobei $D$ die Dianale, $L$ die untere (linke) Hälfte und $R$ die obere (rechte) Hälfte der Matrix $A$ sind, erhalten wir einen Spezialfall der Splitting-Vefahren. \\
Durch die Wahl $M=D$ und $N=L+R$ ergibt sich $x^{(k+1)}=D^{-1}(b - (L+R)x^{(k)})$, bzw. in algorithmischer Form:\\ \\
\algobox{Jacobi / Gesamtschritt Verfahren}{
    Gegeben sei das Lineare Gleichungssystem $Ax=b$ mit $a_{ii}\neq 0$. \\
    \begin{algorithm}[H]
        \SetAlgoNoLine
        \InitTe{Wähle beliebigen Startvektor $x^{(0)}\in\mathbb{K}^n$}{}
        \SetAlgoLined
        \ForTe{$k=1,0,\dotsc$}{
            \ForTe{$i=1,\dotsc,n$}{
                $x_i^{(k+1)}\leftarrow \dfrac{1}{a_{ii}}\left(b_i - \displaystyle\sum_{i\neq j}a_{ij}x_j^{(k)}\right)$
            } \EndTe{}{}
        } 
        \SetAlgoNoLine
        \UntilTe{stop (beliebiges Stopkriterium)}{}
    \end{algorithm}
}
Die zugehörige Iterationsmatrix ist hierbei $J=M^{-1}N = D^{-1}(L+R)$ und nennt sich (beim Jacobi Verfahren) Gesamtschrittoperator. \\ \\
Einen weitere Version des Splitting-Verfahren ergibt sich durch die Wahl $M=D-L$ und $N=R$.
Hierbei bildet $D-L$ eine obere Dreiecksmatrix und die Inversion ergibt sich mittels Vorwärtssubstitution: \\ \\
\algobox{Gauss-Seidel / Einzelschritt Verfahren}{
    Gegeben sei das Lineare Gleichungssystem $Ax=b$ mit $a_{ii}\neq 0$. \\
    \begin{algorithm}[H]
        \SetAlgoNoLine
        \InitTe{Wähle beliebigen Startvektor $x^{(0)}\in\mathbb{K}^n$}{}
        \SetAlgoLined
        \ForTe{$k=1,0,\dotsc$}{
            \ForTe{$i=1,\dotsc,n$}{
                $x_i^{(k+1)}\leftarrow \dfrac{1}{a_{ii}}\left(b_i - \displaystyle\sum_{j<i}a_{ij}x_j^{(k+1)}\sum_{j>i}a_{ij}x_j^{(k)}\right)$
            } \EndTe{}{}
        } 
        \SetAlgoNoLine
        \UntilTe{stop (beliebiges Stopkriterium)}{}
    \end{algorithm}
}
Die hier erhaltene Iterationsmatrix nennen wir Einzelschrittoperator $L=(D-L)^{-1}R$
Mittels der Zeilensumennorm erhalten wir nun ein leicht prüfbares Konvergenzkriterium:
\begin{thmbox}{Satz}
    Ist $A\in\text{GL}_n(\mathbb{K})$ strikt diagonaldominant, d.h. $|a_{ii}| > \sum_{j\neq i} |a_{ij}|$, dann konvergieren Jordan und Gauss-Seidel Verfahren für alle Startwerte $x^{(0)}\in\mathbb{K}^n$ gegen die eindeutige Lösung von $Ax=b$.
\end{thmbox}
\textit{Beweis.} \\
Da $A$ strikt diagonaldominant ist, muss $a_ii \neq 0$ und damit sind beide Verfahren wohldefiniert.
\begin{enumerate}
    \item[a)] Jacobi Verfahren: Für die Iterationsmatrix gilt
    \[\|J\|_\infty = \|D^{-1}(L+R)\|_\infty = \max_{i\in[n]}\dfrac{1}{|a_{ii}|}\sum_{j\neq i}|a_{ij}| =: q < 1\]
    Nach Satz 2.2 folgt damit die Konvergenz des Jacobi Verfahren.
    \item[b)] Gauss-Seidel Verfahren: Um $\|L\|_\infty < 1$ zu zeigen, nutzen wir, dass die Zeilensumennorm die Operatornorm induziert durch die Maximumsnorm ist, d.h.
    \[\|L\|_\infty = \max_{\|x\|_\infty = 1} \|Lx\|_\infty\]
    Sei nun $y=Lx$ für ein $x\in\mathbb{K}^n$ mit $\|x\|_\infty=1$. \\
    Induktiv folgt nun $y_i \le q < 1$, der Induktionsanfang folgt dabei aus dem Beweisteil a). \\
    Unter der Induktionsvoraussetzung gilt für $j<i$, dass $|y_j|\le q$ und damit:
    \begin{align*}
        \|y_i\| &\le \dfrac{1}{|a_{ii}|}\left(\sum_{j<i}|a_{ij}|\cdot\underbrace{|y_j|}_{\le q}+\sum_{j>i}|a_{ij}|\cdot\underbrace{|x_j|}_{\le \|x\|_\infty}\right) \\
        &\le \dfrac{1}{|a_{ii}|}\left(\sum_{j<i}|a_{ij}|\cdot q+\sum_{j>i}|a_{ij}|\cdot \|x\|_\infty\right) \\
        & < \dfrac{1}{|a_{ii}|}\left(\sum_{j<i}|a_{ij}|+\sum_{j>i}|a_{ij}|\right) \\
        & = \dfrac{1}{|a_{ii}|}\sum_{j\neq i} |a_{ij}| \\
        & = q
    \end{align*}
    Da dies für alle Einträge von $y$ gilt folgt $\|y\|_\infty = \|Lx\|_\infty \leq q$ für alle $x$ mit $\|x\|_\infty=1$ und damit $\|L\|_\infty \leq q < 1$ 
    \qed
\end{enumerate}
\begin{egbox}
    Gegeben sei das LGS $Ax=b$ mit 
    \[A = \begin{pmatrix}
        2 & 0 & 1 \\ 1 & -4 & 1 \\ 0 & -1 & 2
    \end{pmatrix}, 
    \quad b=\begin{pmatrix}
        1 \\ 4 \\ -1
    \end{pmatrix}\]
    Dieses System hat die eindeutige Lösung $x^* = (1, -1, -1)^T$. \\
    Durch die Wahl $x^{(0)}=(1,1,1)^T$ erhalten wir beim Jacobi Verfahren:
    \begin{align*}
        x^{(1)} &= D^{-1}(b-(L+R)x^{(0)}) = \begin{pmatrix}
            \tfrac{1}{2} & 0 & 0 \\ 0 & -\tfrac{1}{4} & 0 \\ 0 & 0 & \tfrac{1}{2}
        \end{pmatrix} \cdot \left[\begin{pmatrix}
            1 \\ 4 \\ -1
        \end{pmatrix}-\begin{pmatrix}
            0 & 0 & 1 \\ 1 & 0 & 1 \\ 0 & -1 & 0
        \end{pmatrix}\cdot\begin{pmatrix}
            1 \\ 1 \\ 1
        \end{pmatrix}\right] = \begin{pmatrix}
            0 \\ -\tfrac{1}{2} \\ 0
        \end{pmatrix} \\
        x^{(2)} &= D^{-1}(b-(L+R)x^{(1)}) = \begin{pmatrix}
            \tfrac{1}{2} & 0 & 0 \\ 0 & -\tfrac{1}{4} & 0 \\ 0 & 0 & \tfrac{1}{2}
        \end{pmatrix} \cdot \left[\begin{pmatrix}
            1 \\ 4 \\ -1
        \end{pmatrix}-\begin{pmatrix}
            0 & 0 & 1 \\ 1 & 0 & 1 \\ 0 & -1 & 0
        \end{pmatrix}\cdot\begin{pmatrix}
            0 \\ -\tfrac{1}{2} \\ 0
        \end{pmatrix}\right] = \begin{pmatrix}
            \tfrac{1}{2} \\ -1 \\ -\tfrac{3}{4}
        \end{pmatrix} \\
        \vdots\quad&
    \end{align*}
\end{egbox}
\subsection{Gradientenverfahren}
\textbf{Motivation:} Eine Funtion $f:\mathbb{R}^n\rightarrow \mathbb{R}$ soll minimiert werden. 
Von einem Startpunkt $x^{(0)}$ ausgehen bewegen wir uns nun Stück für Stück in Richtung des steilsten Abstiegs, intuitiv sollten wir so ein Minimum finden. \\
Als Iterationsvorschrift ergibt sich $x^{(k+1)} = x^{(k)}+\alpha^{(k)}\cdot d^{(k)}, \quad k=0,1,\dotsc$\\
dabei ist $\alpha^{(k)}>0$ die Schrittweite und Abstriegsrichtung $d^{(k)}\in\mathbb{R}^n$.
(Eine typische Wahl der Abstriegsrichtung ist $d^{(k)}=-\partial f/\partial x (x^{(k)})=-\nabla f(x^{(k)})$) \\
Das Ziel ist des Verfahren ist es, dass sich der Wert von $f$ in jedem Schritt verbessert, d.h. $f(x^{(k+1)})<f(x^{(k)})$. 
Es ergibt sich ein 1-dim. Optimierungsproblem für die Schrittweite $\alpha^{(k)}$:
\[\alpha^{(k+1)}=\min_{\alpha\neq 0}\{f(x^{(k)}+\alpha\cdot d^{(k)})\}\]
Ein Nachteil des Verfahren ist, dass ein sogenannter \glqq Zick-Zack-Kurs\grqq{} entstehen kann. \\ \\
\textbf{Verfahren der konjugierten Gradienten:} Die obige Idee kann zur effizienten Lösung linearer Gleichungssysteme genutzt werden.
Gegeben sei das LGS $Ax=b$ mit (zunächst) $A\in\mathbb{R}^{n\times n}$ hermitisch, d.h. $a_{ij}=\overline{a_{ji}}$ (hieraus folgt inbesondere, dass die Hauptdiagonale reell ist). 
Zur Lösung wird hierbei die Minimierung des quaratischen Funktionals 
\[\phi(x)=\tfrac{1}{2}x^* Ax - x^*b\]
Sollte eine Lösung $\hat{x}=A^{-1}b$ des LGS $Ax=b$ exisitieren, so gilt für alle $x\in\mathbb{R}^{n\times n}$:
\begin{align*}
    \phi(x)-\phi(\hat{x}) &= \tfrac{1}{2}x^* Ax - x^*b - (\tfrac{1}{2}\hat{x}^* A\hat{x} - \hat{x}^*b) \\
    &\ \ \vdots \\
    &= \tfrac{1}{2} (x-\hat{x})^*A(x-\hat{x}) \geq 0
\end{align*}
Die Funktion hat demnach ein eindeutiges Minimum bei $\hat{x}$.
\begin{defbox}
    Ist $A\in\mathbb{R}^{n\times n}$ hermitisch und pos. definitiv, dann wird durch $\|x\|_A=\sqrt{x^*Ax}, x\in\mathbb{R}^{n\times n}$ eine Norm in $\mathbb{R}^n$ definiert, die sogenannte Energienorm. 
    Zur Energienorm gehört ein inneres Produkt, nämlich $\langle x,y\rangle_A=x^*Ay, x,y\in\mathbb{R}^n$.
    Mithilfe dieser Definition und obiger Erkentniss ergibt sich die Abweichung des Funktionals von seinem Minimum:
    \[\phi(x)-\phi(\hat{x}) = \tfrac{1}{2}\|x-\hat{x}\|^2_A\]
\end{defbox}
\textbf{geometrische Interpretation:} Der Graph von $\phi$ bezüglich der Energienorm ist ein kreisförmiger Parabloid, welcher über dem Mittelpunkt $\hat{x}$ liegt. \\
\textbf{Idee:} Konstruktion eines Verfahrens, welches die Lösung $\hat{x}$ von $Ax=b$ iterativ approximiert, indem das Funktional $\phi$ zukzessiv minimiert wird. Es gilt
\[\phi(x^{(k)}+\alpha d^{(k)}) = \phi(x^{(k)}) + \alpha d^{(k)}A x^{(k)} + \tfrac{1}{2}\alpha^2 {d^{(k)}}^*Ad^{(k)}-2{d^{(k)}}^*\cdot b \tag{4}\]
Durch Differentiation und Null setzen der Ableitung ergibt sich die Schrittweite $\alpha^{(k)}$:
\[\alpha^{(k)}=\dfrac{{r^{(k)}}^* d^{(k)}}{{d^{(k)}}^* A d^{(k)}},\qquad \text{mit } r^{(k)}=b-Ax^{(k)} \tag{5}\]
Weiter ergibt sich die Suchrichtung $d^{(k+1)}$:
\begin{align*}
    d^{(k+1)}=r^{(k+1)} + \beta^{(k)}d^{(k)},\tag{6}\\
    \text{mit } \beta^{(k)} = -\dfrac{r^{(k+1)}Ad^{(k)}}{d^{(k)}Ad^{(k)}} \tag{7}
\end{align*}
Die Gleichungen (5) und (7) sind wohldefiniert, wenn ${d^{(k)}}^*Ad^{(k)}\neq 0$, aufgrund der positiv Definitheit von $A$ ist dies genau dann der Fall wenn $d^{(k)}\neq 0$. 
Nach (6) ist $d^{(k)} = 0$ jedoch nur dann möglich, wenn $r^(k)$ und $d^{(k-1)}$ linear abhängig sind, doch nach Definition verläuft die Suchrichtung tangential zur Niveaufläche von $\phi$, also orthogonal zum Gradienten $r^{(k)}$.
Somit folgt $d^{(k)} = 0$ nur wenn $r^{(k)}=0$, was $x^{(k)}=\hat{x}$ implizieren würde. \\ \\
Wegen der zusätzlichen Orthogonalitätsbedingung $\langle d^{(k+1)},d^{(k)}\rangle_A=0$ nennt man die Suchrichtungen zueinander $A$-konjugiert und das Verfahren, Verfahren der konjugierten Gradienten (CG-Verfahren). 
\end{document} 
